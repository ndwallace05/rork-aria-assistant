import type { LocalLLMModel } from '@/types/localLLM';

export const LOCAL_LLM_MODELS: LocalLLMModel[] = [
  {
    id: 'gemma3n-e2b',
    name: 'Gemma 3N 2B',
    description: 'Google\'s latest efficient 2B model',
    size: 1.6 * 1024 * 1024 * 1024,
    sizeFormatted: '1.6 GB',
    quantization: 'Q4',
    parameters: '2B',
    contextLength: 8192,
    downloadUrl: 'https://huggingface.co/bartowski/gemma-3n-2b-it-GGUF/resolve/main/gemma-3n-2b-it-Q4_K_M.gguf',
    family: 'gemma',
    capabilities: ['Chat', 'Instruction Following', 'Safety'],
    recommended: true,
    minRAM: 3,
  },
  {
    id: 'gemma3n-e4b',
    name: 'Gemma 3N 4B',
    description: 'Google\'s latest balanced 4B model',
    size: 2.8 * 1024 * 1024 * 1024,
    sizeFormatted: '2.8 GB',
    quantization: 'Q4',
    parameters: '4B',
    contextLength: 8192,
    downloadUrl: 'https://huggingface.co/bartowski/gemma-3n-4b-it-GGUF/resolve/main/gemma-3n-4b-it-Q4_K_M.gguf',
    family: 'gemma',
    capabilities: ['Chat', 'Reasoning', 'Code', 'Safety'],
    recommended: true,
    minRAM: 4,
  },
  {
    id: 'gemma3-270m',
    name: 'Gemma 3 270M',
    description: 'Google\'s ultra-compact model for mobile',
    size: 0.2 * 1024 * 1024 * 1024,
    sizeFormatted: '200 MB',
    quantization: 'Q4',
    parameters: '270M',
    contextLength: 8192,
    downloadUrl: 'https://huggingface.co/bartowski/gemma-3-270m-it-GGUF/resolve/main/gemma-3-270m-it-Q4_K_M.gguf',
    family: 'gemma',
    capabilities: ['Chat', 'Instruction Following'],
    recommended: true,
    minRAM: 1,
  },
  {
    id: 'gemma3-1b',
    name: 'Gemma 3 1B',
    description: 'Google\'s compact 1B model',
    size: 0.8 * 1024 * 1024 * 1024,
    sizeFormatted: '800 MB',
    quantization: 'Q4',
    parameters: '1B',
    contextLength: 8192,
    downloadUrl: 'https://huggingface.co/bartowski/gemma-3-1b-it-GGUF/resolve/main/gemma-3-1b-it-Q4_K_M.gguf',
    family: 'gemma',
    capabilities: ['Chat', 'Instruction Following', 'Safety'],
    recommended: true,
    minRAM: 2,
  },
  {
    id: 'phi4-mini-reasoning',
    name: 'Phi-4 Mini Reasoning 3.8B',
    description: 'Microsoft\'s reasoning-focused 3.8B model',
    size: 2.4 * 1024 * 1024 * 1024,
    sizeFormatted: '2.4 GB',
    quantization: 'Q4',
    parameters: '3.8B',
    contextLength: 16384,
    downloadUrl: 'https://huggingface.co/bartowski/phi-4-mini-reasoning-GGUF/resolve/main/phi-4-mini-reasoning-Q4_K_M.gguf',
    family: 'phi',
    capabilities: ['Chat', 'Reasoning', 'Code', 'Math'],
    recommended: true,
    minRAM: 4,
  },
  {
    id: 'granite3.3-2b',
    name: 'Granite 3.3 2B',
    description: 'IBM\'s efficient 2B model',
    size: 1.5 * 1024 * 1024 * 1024,
    sizeFormatted: '1.5 GB',
    quantization: 'Q4',
    parameters: '2B',
    contextLength: 8192,
    downloadUrl: 'https://huggingface.co/bartowski/granite-3.3-2b-instruct-GGUF/resolve/main/granite-3.3-2b-instruct-Q4_K_M.gguf',
    family: 'granite',
    capabilities: ['Chat', 'Code', 'Instruction Following'],
    recommended: true,
    minRAM: 3,
  },
  {
    id: 'cogito-3b',
    name: 'Cogito 3B',
    description: 'Reasoning-optimized 3B model',
    size: 1.9 * 1024 * 1024 * 1024,
    sizeFormatted: '1.9 GB',
    quantization: 'Q4',
    parameters: '3B',
    contextLength: 8192,
    downloadUrl: 'https://huggingface.co/bartowski/cogito-3b-GGUF/resolve/main/cogito-3b-Q4_K_M.gguf',
    family: 'cogito',
    capabilities: ['Chat', 'Reasoning', 'Instruction Following'],
    recommended: true,
    minRAM: 3,
  },
  {
    id: 'deepscaler-1.5b',
    name: 'DeepScaler 1.5B',
    description: 'Efficient 1.5B model for mobile',
    size: 1.0 * 1024 * 1024 * 1024,
    sizeFormatted: '1.0 GB',
    quantization: 'Q4',
    parameters: '1.5B',
    contextLength: 8192,
    downloadUrl: 'https://huggingface.co/bartowski/deepscaler-1.5b-GGUF/resolve/main/deepscaler-1.5b-Q4_K_M.gguf',
    family: 'deepscaler',
    capabilities: ['Chat', 'Instruction Following'],
    recommended: true,
    minRAM: 2,
  },
  {
    id: 'deepseek-r1-1.5b',
    name: 'DeepSeek R1 1.5B',
    description: 'DeepSeek\'s reasoning model',
    size: 1.0 * 1024 * 1024 * 1024,
    sizeFormatted: '1.0 GB',
    quantization: 'Q4',
    parameters: '1.5B',
    contextLength: 8192,
    downloadUrl: 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-Q4_K_M.gguf',
    family: 'deepseek',
    capabilities: ['Chat', 'Reasoning', 'Code'],
    recommended: true,
    minRAM: 2,
  },
  {
    id: 'llama3.2-1b',
    name: 'Llama 3.2 1B',
    description: 'Meta\'s latest compact model',
    size: 0.8 * 1024 * 1024 * 1024,
    sizeFormatted: '800 MB',
    quantization: 'Q4',
    parameters: '1B',
    contextLength: 8192,
    downloadUrl: 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_K_M.gguf',
    family: 'llama',
    capabilities: ['Chat', 'Instruction Following'],
    recommended: true,
    minRAM: 2,
  },
  {
    id: 'qwen3-0.6b',
    name: 'Qwen 3 0.6B',
    description: 'Alibaba\'s ultra-compact multilingual model',
    size: 0.5 * 1024 * 1024 * 1024,
    sizeFormatted: '500 MB',
    quantization: 'Q4',
    parameters: '0.6B',
    contextLength: 32768,
    downloadUrl: 'https://huggingface.co/Qwen/Qwen3-0.6B-Instruct-GGUF/resolve/main/qwen3-0.6b-instruct-q4_k_m.gguf',
    family: 'qwen',
    capabilities: ['Chat', 'Multilingual'],
    recommended: true,
    minRAM: 1,
  },
  {
    id: 'qwen3-1.7b',
    name: 'Qwen 3 1.7B',
    description: 'Alibaba\'s efficient multilingual model',
    size: 1.2 * 1024 * 1024 * 1024,
    sizeFormatted: '1.2 GB',
    quantization: 'Q4',
    parameters: '1.7B',
    contextLength: 32768,
    downloadUrl: 'https://huggingface.co/Qwen/Qwen3-1.7B-Instruct-GGUF/resolve/main/qwen3-1.7b-instruct-q4_k_m.gguf',
    family: 'qwen',
    capabilities: ['Chat', 'Multilingual', 'Code'],
    recommended: true,
    minRAM: 2,
  },
];

export const getRecommendedModels = (): LocalLLMModel[] => {
  return LOCAL_LLM_MODELS.filter(model => model.recommended);
};

export const getModelsByFamily = (family: LocalLLMModel['family']): LocalLLMModel[] => {
  return LOCAL_LLM_MODELS.filter(model => model.family === family);
};

export const getModelById = (id: string): LocalLLMModel | undefined => {
  return LOCAL_LLM_MODELS.find(model => model.id === id);
};

export const getModelsBySize = (maxSizeGB: number): LocalLLMModel[] => {
  const maxSizeBytes = maxSizeGB * 1024 * 1024 * 1024;
  return LOCAL_LLM_MODELS.filter(model => model.size <= maxSizeBytes);
};
